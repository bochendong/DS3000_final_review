{"cells":[{"cell_type":"markdown","metadata":{"id":"E-qkQay4t_EU"},"source":["# Global Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbOpeHaUt_El"},"outputs":[],"source":["# Libraries:\n","import os\n","import numpy as np\n","import sklearn\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","# Some plot styling:\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)"]},{"cell_type":"markdown","metadata":{"id":"WN0EmhYkt_Eu"},"source":["# Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"-7wXVxBot_Ey"},"source":["## Normal Equation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0A7f-zO4t_E3"},"outputs":[],"source":["# Recap: \"numpy.c_\" translates slice objects to concatenation along the second axis.\n","# Example:\n","a = np.array([1,2,3])\n","b = np.array([6,7,8])\n","print('a=', a)\n","print('b=', b)\n","np.c_[a,b]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JGaWg4Et_E6"},"outputs":[],"source":["# Let's set a seed:\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 1) \n","# PS. \"rand\" samples from uniform distribution in the range 0 to 1\n","print(X[0])\n","\n","# Add x0 = 1 to each instance to make it suitable for the dot product\n","X_b = np.c_[np.ones((100, 1)), X]\n","print(X_b[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEopDDeZt_E9"},"outputs":[],"source":["y = 4 + 3 * X + np.random.randn(100, 1)\n","# PS. \"randn\" samples from normal distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yt1VILs5t_FC"},"outputs":[],"source":["plt.plot(X, y, \"b.\")\n","plt.xlabel(\"$x_1$\", fontsize=18)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.axis([0, 2, 0, 15])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDfZhqcht_FF"},"outputs":[],"source":["# Compute theta hat using the normal equation\n","theta_hat = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n","theta_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjzXZBvHt_FM"},"outputs":[],"source":["# Now let's use theta_hat to make a prediction\n","\n","# Create new instance\n","X_new = np.array([[0], [2]])\n","X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n","X_new"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dabbD8kBt_FS"},"outputs":[],"source":["y_predict = X_new_b.dot(theta_hat)\n","y_predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4tlmfyut_FX"},"outputs":[],"source":["plt.plot(X_new, y_predict,'r-')\n","plt.plot(X, y, \"b.\")\n","plt.axis([0, 2, 0, 15])\n","plt.xlabel(\"$x_1$\", fontsize=18)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ine2wW1qt_Fa"},"source":["## Singular Value Decomposition (SVD)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7j5nYcAt_Fc"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","lin_reg = LinearRegression()\n","lin_reg.fit(X, y)\n","\n","lin_reg.intercept_, lin_reg.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_3J0e09t_Ff"},"outputs":[],"source":["lin_reg.predict(X_new)"]},{"cell_type":"markdown","metadata":{"id":"BA81wmWjt_Fi"},"source":["# Gradient Descent\n","## Batch Gradient Descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gj0ZJ5Yut_Fk"},"outputs":[],"source":["eta = 0.1  # learning rate\n","n_iterations = 1000\n","m = len(X_b)\n","\n","theta = np.random.randn(2,1)  # random initialization\n","\n","for iteration in range(n_iterations):\n","    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n","    theta = theta - eta * gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpDwk7nft_Fn"},"outputs":[],"source":["theta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Y3abCo2t_Fq"},"outputs":[],"source":["X_new_b.dot(theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6en2htjt_F_"},"outputs":[],"source":["def plot_gradient_descent(theta, eta, theta_path=None):\n","    m = len(X_b)\n","    plt.plot(X, y, \"b.\")\n","    n_iterations = 1000\n","    for iteration in range(n_iterations):\n","        if iteration < 10:\n","            y_predict = X_new_b.dot(theta)\n","            style = \"b-\" if iteration > 0 else \"r--\"\n","            plt.plot(X_new, y_predict, style)\n","        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n","        theta = theta - eta * gradients\n","        if theta_path is not None:\n","            theta_path.append(theta)\n","    plt.xlabel(\"$x_1$\", fontsize=18)\n","    plt.axis([0, 2, 0, 15])\n","    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTF5GTOit_GE"},"outputs":[],"source":["np.random.seed(42)\n","theta = np.random.randn(2,1)  # random initialization\n","\n","plt.figure(figsize=(10,4))\n","plt.subplot(131); plot_gradient_descent(theta, eta=0.02); plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.subplot(132); plot_gradient_descent(theta, eta=0.10)\n","plt.subplot(133); plot_gradient_descent(theta, eta=0.50)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RjPh92Zot_GI"},"source":["## Stochastic Gradient Descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqglGt3gt_GJ"},"outputs":[],"source":["m = len(X_b)\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOBmDpeSt_GM"},"outputs":[],"source":["n_epochs = 50\n","\n","# We want to gradually reduce the learning rate. The steps start out large (which helps make\n","# quick progress and escape local minima), then get smaller and smaller. The \"learning schedule\" \n","# determines the learning rate at each iteration.\n","t0, t1 = 5, 50  # learning schedule hyperparameters\n","def learning_schedule(t):\n","    return t0 / (t + t1)\n","\n","theta = np.random.randn(2,1)  # random initialization\n","\n","for epoch in range(n_epochs):\n","    for i in range(m):\n","        if epoch == 0 and i < 20:                    \n","            y_predict = X_new_b.dot(theta)           \n","            style = \"b-\" if i > 0 else \"r--\"         \n","            plt.plot(X_new, y_predict, style)        \n","        random_index = np.random.randint(m)\n","        xi = X_b[random_index:random_index+1]\n","        yi = y[random_index:random_index+1]\n","        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n","        eta = learning_schedule(epoch * m + i)\n","        theta = theta - eta * gradients                \n","\n","plt.plot(X, y, \"b.\")                                 \n","plt.xlabel(\"$x_1$\", fontsize=18)                     \n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)           \n","plt.axis([0, 2, 0, 15])                              \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orAr1xRjt_GP","scrolled":true},"outputs":[],"source":["theta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZI2rYo4it_GR"},"outputs":[],"source":["from sklearn.linear_model import SGDRegressor\n","\n","sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.1, random_state=42) \n","sgd_reg.fit(X, y.ravel()) # \"ravel\" to flatten the array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_vvpdHTt_GZ"},"outputs":[],"source":["sgd_reg.intercept_, sgd_reg.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vfNbKa1t_Gc"},"outputs":[],"source":["score = sgd_reg.score(X, y.ravel())\n","print(\"R-squared:\", score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WWUlsk8t_Gc"},"outputs":[],"source":["sgd_reg.predict(X_new)"]},{"cell_type":"markdown","metadata":{"id":"fZq-gP3ut_Ge"},"source":["## Mini-batch gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlf1vLcot_Ge"},"outputs":[],"source":["n_iterations = 50\n","minibatch_size = 20\n","\n","np.random.seed(42)\n","theta = np.random.randn(2,1)  # random initialization\n","\n","t0, t1 = 200, 1000\n","def learning_schedule(t):\n","    return t0 / (t + t1)\n","\n","t = 0\n","for epoch in range(n_iterations):\n","    shuffled_indices = np.random.permutation(m)\n","    X_b_shuffled = X_b[shuffled_indices]\n","    y_shuffled = y[shuffled_indices]\n","    for i in range(0, m, minibatch_size):\n","        t += 1\n","        xi = X_b_shuffled[i:i+minibatch_size]\n","        yi = y_shuffled[i:i+minibatch_size]\n","        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n","        eta = learning_schedule(t)\n","        theta = theta - eta * gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMiKst94t_Gg"},"outputs":[],"source":["theta"]},{"cell_type":"markdown","metadata":{"id":"mTcOzwl_t_Gg"},"source":["# Polynomial Regression\n","We said we can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bx54iQYt_Gh"},"outputs":[],"source":["# Let’s generate some nonlinear data, based on a quadratic equation\n","import numpy.random as rnd\n","np.random.seed(42)\n","\n","m = 100\n","X = 6 * np.random.rand(m, 1) - 3\n","y = X + 0.5 * X**2 + 2 + np.random.randn(m, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UZV0gszt_Gn"},"outputs":[],"source":["plt.plot(X, y, \"b.\")\n","plt.xlabel(\"$x_1$\", fontsize=18)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.axis([-3, 3, 0, 10])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4k3jDsWgt_Gq"},"outputs":[],"source":["# Use Scikit-Learn’s PolynomialFeatures class to transform the training data\n","from sklearn.preprocessing import PolynomialFeatures\n","poly_features = PolynomialFeatures(degree=2, include_bias=False)\n","X_poly = poly_features.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3392sEJvt_Gv"},"outputs":[],"source":["X[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XHKRbixt_Gy"},"outputs":[],"source":["X_poly[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvLa9AEkt_G1"},"outputs":[],"source":["X[0]*X[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPIM9vp6t_G2"},"outputs":[],"source":["lin_reg = LinearRegression()\n","lin_reg.fit(X_poly, y)\n","lin_reg.intercept_, lin_reg.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQB6TaFtt_G4"},"outputs":[],"source":["X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n","X_new_poly = poly_features.transform(X_new)\n","y_new = lin_reg.predict(X_new_poly)\n","plt.plot(X, y, \"b.\")\n","plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n","plt.xlabel(\"$x_1$\", fontsize=18)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plt.axis([-3, 3, 0, 10])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eBGiO-05t_G6"},"source":["# Learning Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMXby4kft_G8"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","def plot_learning_curves(model, X, y):\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n","    train_errors, val_errors = [], []\n","    for m in range(1, len(X_train) + 1):\n","        model.fit(X_train[:m], y_train[:m])\n","        y_train_predict = model.predict(X_train[:m])\n","        y_val_predict = model.predict(X_val)\n","        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n","        val_errors.append(mean_squared_error(y_val, y_val_predict))\n","\n","    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n","    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n","    plt.legend(loc=\"upper right\", fontsize=14)\n","    plt.xlabel(\"Training set size\", fontsize=14)\n","    plt.ylabel(\"RMSE\", fontsize=14)              "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tfxezj9ht_G9"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","\n","polynomial_regression = Pipeline([\n","        (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)), # play with degree\n","        (\"lin_reg\", LinearRegression()),\n","    ])\n","\n","plot_learning_curves(polynomial_regression, X, y)\n","plt.axis([0, 80, 0, 3])\n","plt.show()         "]},{"cell_type":"markdown","metadata":{"id":"hnSJ2eL9t_G_"},"source":["# Regularized Linear Models"]},{"cell_type":"markdown","metadata":{"id":"uyJh2ZLft_HG"},"source":["## Ridge Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7302KrQt_HI"},"outputs":[],"source":["np.random.seed(42)\n","m = 20\n","X = 3 * np.random.rand(m, 1)\n","y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n","X_new = np.linspace(0, 3, 100).reshape(100, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfSuz6gpt_HK"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n","ridge_reg.fit(X, y)\n","ridge_reg.predict([[1.5]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKEWTKGIt_HN"},"outputs":[],"source":["ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n","ridge_reg.fit(X, y)\n","ridge_reg.predict([[1.5]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r20FhxYRt_HO"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","from sklearn.preprocessing import StandardScaler\n","\n","def plot_model(model_class, polynomial, alphas, **model_kargs):\n","    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n","        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n","        if polynomial:\n","            model = Pipeline([\n","                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n","                    (\"std_scaler\", StandardScaler()),\n","                    (\"regul_reg\", model),\n","                ])\n","        model.fit(X, y)\n","        y_new_regul = model.predict(X_new)\n","        lw = 2 if alpha > 0 else 1\n","        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n","    plt.plot(X, y, \"b.\", linewidth=3)\n","    plt.legend(loc=\"upper left\", fontsize=15)\n","    plt.xlabel(\"$x_1$\", fontsize=18)\n","    plt.axis([0, 3, 0, 4])\n","\n","plt.figure(figsize=(8,4))\n","plt.subplot(121)\n","plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.subplot(122)\n","plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVysvSwrt_HQ"},"outputs":[],"source":["sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n","sgd_reg.fit(X, y.ravel())\n","sgd_reg.predict([[1.5]])"]},{"cell_type":"markdown","metadata":{"id":"2Kgl1PQOt_HR"},"source":["## Lasso Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFozvzUPt_HS"},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","\n","plt.figure(figsize=(8,4))\n","plt.subplot(121)\n","plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n","plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n","plt.subplot(122)\n","plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmSATwXQt_HV"},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","lasso_reg = Lasso(alpha=0.1)\n","lasso_reg.fit(X, y)\n","lasso_reg.predict([[1.5]])"]},{"cell_type":"markdown","metadata":{"id":"mms90444t_HY"},"source":["## Elastic Net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QAUk2i_t_HZ"},"outputs":[],"source":["from sklearn.linear_model import ElasticNet\n","elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n","elastic_net.fit(X, y)\n","elastic_net.predict([[1.5]])"]},{"cell_type":"markdown","metadata":{"id":"EeYwfMW6t_Hc"},"source":["If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex."]}],"metadata":{"colab":{"collapsed_sections":["RjPh92Zot_GI","fZq-gP3ut_Ge"],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"nav_menu":{},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
