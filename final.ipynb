{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.optimize as so\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error # Requires sklearn 0.24 (December 2020), update with conda/pip if needed.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import make_scorer\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aug    35.59\n",
       "sep    33.27\n",
       "mar    10.44\n",
       "jul     6.19\n",
       "feb     3.87\n",
       "jun     3.29\n",
       "oct     2.90\n",
       "apr     1.74\n",
       "dec     1.74\n",
       "jan     0.39\n",
       "may     0.39\n",
       "nov     0.19\n",
       "Name: month, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df.month.value_counts()/df.month.value_counts().sum())*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group insignificant labels into two new statistically significant labels.\n",
    "df.month.replace({'feb':'fj', 'jun':'fj'}, inplace=True)\n",
    "\n",
    "df.month.replace({'oct':'oadjmn', 'apr':'oadjmn', 'dec':'oadjmn', \n",
    "    'jan':'oadjmn', 'may':'oadjmn', \n",
    "    'nov':'oadjmn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aug       35.59\n",
       "sep       33.27\n",
       "mar       10.44\n",
       "oadjmn     7.35\n",
       "fj         7.16\n",
       "jul        6.19\n",
       "Name: month, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df.month.value_counts()/df.month.value_counts().sum())*100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Convert\n",
    "convert all categorical data into numerical data using `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = Pipeline([\n",
    "    ('lr1', LinearRegression())\n",
    "])\n",
    "\n",
    "M2 = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('lr3', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Temperature (temp)` and `Rain (rain)` may be important features, so let's extend model 1 by adding a *cubed* term for temp and a *squared* term for rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.assign(temp2 = X.temp**3)\n",
    "        X = X.assign(rain2 = X.rain**2)\n",
    "        return X\n",
    "\n",
    "# Create a pipeline for model 3 (M3) [ /8 marks]\n",
    "M3 = Pipeline([\n",
    "    ('temp_cubic_rain_square', KeyFeatures()),\n",
    "    ('lr2', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-flod cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, ypr):\n",
    "    return np.mean((y-ypr)**2)\n",
    "    \n",
    "kf = KFold(n_splits=4)\n",
    "sc = make_scorer(MSE) \n",
    "\n",
    "cvsc1 = cross_val_score(M1, Xtrain, ytrain, cv=kf, scoring=sc)\n",
    "cvsc2 = cross_val_score(M2, Xtrain, ytrain, cv=kf, scoring=sc)\n",
    "cvsc3 = cross_val_score(M3, Xtrain, ytrain, cv=kf, scoring=sc)\n",
    "\n",
    "print(f\"M1 loss: %.4f +/- %.4f\" % (cvsc1.mean(), cvsc1.std()))\n",
    "print(f\"M2 loss: %.4f +/- %.4f\" % (cvsc2.mean(), cvsc2.std()))\n",
    "print(f\"M3 loss: %.4f +/- %.4f\" % (cvsc3.mean(), cvsc3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('energy_appliances_standard.csv')\n",
    "y = df[\"Appliances\"]\n",
    "X =  df.drop(\"Appliances\",axis=1)\n",
    "RANDOM_STATE = 20201107\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.3, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = RandomForestRegressor(\n",
    "            n_estimators = 250,\n",
    "            max_features=None,\n",
    "            oob_score=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "ff.fit(Xtrain, ytrain)\n",
    "\n",
    "# Calculate error over test set\n",
    "y_pred = ff.predict(Xtest)\n",
    "err_test = mean_absolute_percentage_error(y_pred, ytest)\n",
    "\n",
    "print(\"MAPE (test set): %f\" % err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('energy_appliances_standard.csv')\n",
    "y = df[\"Appliances\"]\n",
    "X =  df.drop(\"Appliances\",axis=1)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.3, random_state = RANDOM_STATE)\n",
    "\n",
    "XGB_opt = XGBRegressor(learning_rate = 0.1,  \n",
    "                            max_depth =  7,\n",
    "                            n_estimators = 450,\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='reg:squarederror',       # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=-1,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=RANDOM_STATE        # Seed\n",
    "                            )\n",
    "\n",
    "\n",
    "XGB_opt.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = XGB_opt.predict(Xtest)\n",
    "err_test_xgb = mean_absolute_percentage_error(y_pred_xgb, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance\n",
    "importances = XGB_opt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[Xtrain.columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
